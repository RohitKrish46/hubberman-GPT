# huberman-GPT
This repository contains the implementation of a character-level transformer model for text generation, inspired by Andrew Huberman's podcast. The model generates text by learning the patterns and structure of podcast text data at the character level. The model learns to predict the next character in a sequence based on the previous characters, capturing long-range dependencies in the text data.


## Training
```
python train.py
```

## Text Generation:
```
python generate.py
```
